{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Science Workshop @VeloCity\n",
    "\n",
    "\n",
    "\n",
    "## 1. Jupyter Notebook\n",
    "\n",
    "Jupyter notebook is often used by data scientists who work in Python. It is loosely based on Mathematica and combines code, text and visual output in one page.\n",
    "\n",
    "Some relevant short cuts:\n",
    "* ```SHIFT + ENTER``` executes 1 block of code called a cell\n",
    "* Tab-completion is omnipresent after the import of a package has been executed\n",
    "* ```SHIFT + TAB``` gives you extra information on what parameters a function takes\n",
    "* Repeating ```SHIFT + TAB``` multiple times gives you even more information\n",
    "\n",
    "To get used to these short cuts try them out on the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Hello world!'\n",
    "print range(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Anomaly Detection\n",
    "\n",
    "### 2.1 Load Data\n",
    "\n",
    "First we will load the data using a [pickle format](https://docs.python.org/2/library/pickle.html). (We use ```import cPickle as pickle``` because cPickle is faster.)\n",
    "\n",
    "The data we use contains the pageviews of one of our own websites and for convenience there is only 1 data point per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "past = pickle.load(open('data/past_data.pickle'))\n",
    "all_data = pickle.load(open('data/all_data.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Plot past data\n",
    "\n",
    "To plot the past data we will use ```matplotlib.pyplot```. For convenience we import it as ```plt```. \n",
    "```% matplotlib inline``` makes sure you can see the output in the notebook. \n",
    "\n",
    "(Use ```% matplotlib notebook``` if you want to make it ineractive. Don't forget to click the power button to finish the interaction and to be able to plot a new figure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,4)) # This creates a new figure with the dimensions of 20 by 4\n",
    "plt.plot(past)             # This creates the actual plot\n",
    "plt.show()                 # This shows the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Find the minimum and maximum\n",
    "\n",
    "Use [```np.nanmax()```](http://docs.scipy.org/doc/numpy/reference/generated/numpy.nanmax.html#numpy.nanmax) and [```np.nanmin()```](http://docs.scipy.org/doc/numpy/reference/generated/numpy.nanmin.html) to find the minmum and maximum while ignoring the NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maximum = \n",
    "minimum = \n",
    "\n",
    "print minimum, maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot these together with the data using the [```plt.axhline()```](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.axhline) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(past)\n",
    "plt.axhline(maximum, color='r')\n",
    "plt.axhline(minimum, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Testing the model on unseen data\n",
    "\n",
    "Now plot all the data instead of just the past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(all_data, color='g')\n",
    "plt.plot(past, color='b')\n",
    "plt.axhline(maximum, color='r')\n",
    "plt.axhline(minimum, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see now  that this model does not detect any anomalies. However, the last day of data clearly looks different compared to the other days.\n",
    "\n",
    "In what follows we will build a better model for anomaly detection that is able to detect these 'shape shifts' as well.\n",
    "\n",
    "### 2.5 Building a model with seasonality\n",
    "\n",
    "To do this we are going to take a step by step approach. Maybe it won't be clear at first why every step is necessary, but that will become clear throughout the process.\n",
    "\n",
    "First we are going to reshape the past data to a 2 dimensional array with 24 columns. This wil give is 1 row for each day and 1 column for each hour. For this we are going to use the [```np.reshape()```](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) function. The newshape parameter is a tuple which in this case should be ```(-1, 24)```. If you use a ```-1``` the reshape function will automatically compute that dimension. Pay attention to the order in which the numbers are repositonned (the default ordering should work fine here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reshaped_past = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to compute the average over all days. For this we are going to use the [```np.mean()```](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) with the axis variable set to the first dimension (```axis=0```). Next we are going to plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_past = \n",
    "\n",
    "plt.plot(average_past)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can see in the plot above is the average number of pageviews for eacht hour of the day.\n",
    "\n",
    "Now let's plot this together with the past data on 1 plot. Use a [for loop](https://wiki.python.org/moin/ForLoop) and the [```np.concatenate()```](http://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html) function to concatenate this average 6 times into the variable ```model```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = []\n",
    "\n",
    "### concatenate here ###\n",
    "\n",
    "plt.figure(figsize=(20,4))    \n",
    "plt.plot(model, color='k')\n",
    "plt.plot(past, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we are going to compute the maximum (= positive) and minimum (= negative) deviations from the average to determine what kind of deviations are normal. (Just substract the average/model from the past and take the min and the max of that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta_max = \n",
    "delta_min = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(model, color='k')\n",
    "plt.plot(past, color='b')\n",
    "plt.plot(model + delta_max, color='r')\n",
    "plt.plot(model + delta_min, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test this on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_all = np.concatenate((model, average_past))\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(all_data, color='g')\n",
    "plt.plot(model_all, color='k')\n",
    "plt.plot(past, color='b')\n",
    "plt.plot(model_all + delta_max, color='r')\n",
    "plt.plot(model_all + delta_min, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can clearly see where the anomaly is detected by this more advanced model. The code below can gives you the exact indices where an anomaly is detected. The functions uses are the following [```np.where()```](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html) and [```np.logical_or()```](http://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_or.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anomaly_timepoints = np.where(np.logical_or(all_data < model_all + delta_min, all_data > model_all + delta_max))[0]\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.scatter(anomaly_timepoints, all_data[anomaly_timepoints], color='r', linewidth=8)\n",
    "plt.plot(all_data, color='g')\n",
    "plt.plot(model_all, color='k')\n",
    "plt.plot(past, color='b')\n",
    "plt.plot(model_all + delta_max, color='r')\n",
    "plt.plot(model_all + delta_min, color='r')\n",
    "plt.xlim(0, len(all_data))\n",
    "plt.show()\n",
    "\n",
    "print 'The anomaly occurs at the following timestamps:', anomaly_timepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "\n",
    "It is often desired to understand the relationship between different sources of information. As an example we'll consider the historical request rate of a web server and compare it to its CPU usage. We'll try to predict the CPU usage of the server based on the request rates of the different pages. First some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (13.0, 8.0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data import and inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pandas](http://pandas.pydata.org/) is a popular library for data wrangling, we'll use it to load and inspect a csv file that contains the historical web request and cpu usage of a web server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_csv(\"data/request_rate_vs_CPU.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head command allows one to quickly see the structure of the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the CPU column and plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(figsize=(13,8), y=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot the request rates, leaving out the CPU column  as it has another unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop('CPU',1).plot(figsize=(13,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to continue and start to model the data, we'll work with basic numpy arrays.\n",
    "\n",
    "We extract the column labels as the request_names for later reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "request_names = data.drop('CPU',1).columns.values\n",
    "request_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the request rates as a 2-dimensional numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request_rates = data.drop('CPU',1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the cpu usage as a one-dimensional numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpu = data['CPU'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Simple linear regression\n",
    "\n",
    "First, we're going to work with the total request rate on the server, and compare it to the CPU usage. The numpy function [sum](http://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html) can be used to calculate the total request rate when selecting the right direction (axis) for th summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0a43f085135b9beb7e83727ca2d7ac5c",
     "grade": false,
     "grade_id": "1",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the total request rate to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.plot(total_request_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make use of a [PyPlot's scatter plot](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter) to understand the relation between the total request rate and the CPU usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in \n",
    "plt.figure(figsize=(13,8))\n",
    "plt.xlabel(\"Total request rate\")\n",
    "plt.ylabel(\"CPU usage\")\n",
    "plt.scatter( ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There clearly is a strong correlation between the request rate and the CPU usage. Now we'll try to capture this relation using a linear model:\n",
    "$$ \\text{cpu} = c_0 + c_1 \\text{total_request_rate} $$\n",
    "\n",
    "For that we'll make use of the [scikit-learn](http://scikit-learn.org/stable/) machine learning library for Python and use [least-squares linear regression](http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to feed the data to the model to fit it. The model.fit method expects a matrix so we need to convert the total_request_rate into a matrix with one column, we can \n",
    "use the [reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) method for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill in\n",
    "total_request_rate_M = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit our model using the the total request rate and cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fill in\n",
    "model.fit(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the coefficient $c_1$ of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the constant term $c_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained we can use it to [predict](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) the outcome for a given input (or array of inputs). \n",
    "\n",
    "What is the expected CPU usage when we have 50 requests per second? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in \n",
    "model.predict(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the linear model together with our data to verify it captures the relationship correctly (the predict method can accept total_request_rate_M at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter( ... , ...  , color='black')\n",
    "plt.plot( ... , ... , color='blue', linewidth=3)\n",
    "plt.xlabel(\"Total request rate\")\n",
    "plt.ylabel(\"CPU usage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model also has a [score](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score) indicating how well the linear model captures the data. A score of 1 means the data is perfectly linear, a score of 0 (or lower) means the data is not linear at all (and it does not make sense to try to model it that way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.score(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multiple linear regression\n",
    "\n",
    "Now we consider the separate request rates again and build a linear model for that. The model we try to fit takes the form:\n",
    "$$\\text{cpu} = c_0 + c_1 \\text{request_rate}_1 + c_2 \\text{request_rate}_2 + \\ldots + c_n \\text{request_rate}_n$$\n",
    "where the $\\text{request_rate}_i$'s correspond the our different requests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "request_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we create a new [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill in\n",
    "multi_lin_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next fit the model on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in\n",
    "multi_lin_model.fit("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which request causes most CPU usage, on a per visit basis? ([np.argmax](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html) finds the index of the greatest element in an array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fill in \n",
    "heavy_request = \n",
    "print heavy_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to minimize average CPU usage on this server by deviating traffic of one webpage to another server, which page should we choose?  \n",
    "One way to determine this is by using the multi_lin_model.predict method. Another way is by directly using the regression formula. Some functions that might be useful for this:\n",
    "- [np.mean](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html) can used to calculate the mean of the values in a matrix\n",
    "- a * b will calculate the pairwaize product of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in\n",
    "average_rates = \n",
    "request_to_move = \n",
    "print request_to_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Forecasting\n",
    "\n",
    "For the forecasting we are going to use page views data, very similar to the data used in the anomaly detection section. It is also page view data and contains 1 sample per hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pickle.load(open('data/train_set_forecasting.pickle'))\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(train_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above you can clearly see that there is a rising trend in the data.\n",
    "\n",
    "### 4.1 One-step ahead prediction\n",
    "\n",
    "This forecasting section will describe the one-step ahead prediction. This means in this case that we will only predict the next data point which is in this case the number of pageviews in the next hour.\n",
    "\n",
    "Now let's first build a model that tries to predict the next data point from the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.gaussian_process\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# the input X contains all the data except the last data point\n",
    "X = train_set[ : -1].reshape((-1, 1)) # the reshape is necessary since sklearn requires a 2 dimensional array\n",
    "\n",
    "# the output y contains all the data except the first data point\n",
    "y = train_set[1 : ]\n",
    "\n",
    "# this code fits the model on the train data\n",
    "model.fit(X, y)\n",
    "\n",
    "# this score gives you how well it fits on the train set\n",
    "# higher is better and 1.0 is perfect\n",
    "print 'The score of the linear model is', model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the score above, the model is not perfect but it seems to get a relatively high score. Now let's make a prediction into the future and plot this.\n",
    "\n",
    "To predict the datapoint after that we will use the predicted data to make a new prediction. The code below shows how this works for this data set using the linear model you used earlier. Don't forget to fill out the missing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nof_predictions = 100\n",
    "\n",
    "import copy\n",
    "# use the last data point as the first input for the predictions\n",
    "x_test = copy.deepcopy(train_set[-1]) # make a copy to avoid overwriting the training data\n",
    "\n",
    "prediction = []\n",
    "for i in range(nof_predictions):\n",
    "    # predict the next data point\n",
    "    y_test = model.predict(x_test.reshape((1,1))) # the reshape is necessary since sklearn requires a 2 dimensional array\n",
    "    \n",
    "    ##### Complete this part of the code #####\n",
    "    prediction.append()\n",
    "    x_test = \n",
    "    ##########################################\n",
    "    \n",
    "\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((train_set, prediction[:,0])), 'g')\n",
    "plt.plot(train_set, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the image above the model doesn't quite seem to fit the data well. Let's see how we can improve this.\n",
    "\n",
    "### 4.2 Multiple features\n",
    "\n",
    "If your model is not smart enough there is a simple trick in machine learning to make your model more intelligent (but also more complex). This is by adding more features.\n",
    "\n",
    "To make our model better we will use more than 1 sample from the past. To make your life easier there is a simple function below that will create a data set for you. The ```width``` parameter sets the number of hours in the past that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_time_series_to_Xy(ts, width):\n",
    "    X, y = [], []\n",
    "    for i in range(len(ts) - width - 1):\n",
    "        X.append(ts[i : i + width])\n",
    "        y.append(ts[i + width])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 5\n",
    "X, y = convert_time_series_to_Xy(train_set, width)\n",
    "\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the print above both X and y contains 303 datapoints. For X you see that there are now 5 features which contain the pageviews from the 5 past hours.\n",
    "\n",
    "So let's have a look what the increase from 1 to 5 features results to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 5\n",
    "X, y = convert_time_series_to_Xy(train_set, width)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X,y)\n",
    "print 'The score of the linear model with width =', width, 'is', model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the ```width``` parameter to see if you can get a better score.\n",
    "\n",
    "### 4.3 Over-fitting\n",
    "\n",
    "Now execute the code below to see the prediction of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a helper function to make the predictions\n",
    "def predict(model, train_set, width, nof_points):\n",
    "    prediction = []\n",
    "    # create the input data set for the first predicted output\n",
    "    # copy the data to make sure the orriginal is not overwritten\n",
    "    x_test = copy.deepcopy(train_set[-width : ]) \n",
    "    for i in range(nof_points):\n",
    "        # predict only the next data point\n",
    "        prediction.append(model.predict(x_test.reshape((1, -1))))\n",
    "        # use the newly predicted data point as input for the next prediction\n",
    "        x_test[0 : -1] = x_test[1 : ]\n",
    "        x_test[-1] = prediction[-1]\n",
    "    return np.array(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nof_predictions = 200\n",
    "prediction = predict(model, train_set, width, nof_predictions)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((train_set, prediction[:,0])), 'g')\n",
    "plt.plot(train_set, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the image above the prediction is not what you would expect from a perfect model. What happened is that the model learned the training data by hart without 'understanding' what the data is really about. This fenomenon is called over-fitting and will always occur if you make your model more complex.\n",
    "\n",
    "Now play with the number again to see if you can find a more sensible width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = \n",
    "X, y = convert_time_series_to_Xy(train_set, width)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X,y)\n",
    "print 'The score of the linear model with width =', width, 'is', model.score(X, y)\n",
    "\n",
    "prediction = predict(model, train_set, width, 200)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((train_set, prediction[:,0])), 'g')\n",
    "plt.plot(train_set, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will have noticed by now is that it is better to have a non-perfect score which will give you a much better outcome. Now try the same thing for the following models:\n",
    "* ```sklearn.linear_model.RidgeCV()```\n",
    "* ```sklearn.linear_model.LassoCV()```\n",
    "* ```sklearn.gaussian_process.GaussianProcess()```\n",
    "\n",
    "The first 2 models also estimate the noise that is present in the data to avoid overfitting. ```RidgeCV``` will keep the weights that are found small, but it won't put them to zero. ```LassoCV``` on the other hand will put several weights to 0. Execute ```model.coef_``` to see the actual coefficients that have been found.\n",
    "\n",
    "```GaussianProcess``` is a non-linear method. This makes this method a lot more complex and therefore it will need significantly less features to be able to learn the data by hart (and thus to over-fit). In many cases however this additional complexity allows to better understand the data. Additionally it has the advantage that it can estimate confidance intervals similar to the red lines used in the anomaly detection.\n",
    "\n",
    "### 4.4 Automation\n",
    "\n",
    "What we have done up to now is manually selecting the best outcome based on the test result. This can be considered cheating because you have just created a self-fulfilling profecy. Additionally it is not only cheating it is also hard to find the exact ```width``` that gives the best result by just visually inspecting it. So we need a more objective approach to solve this.\n",
    "\n",
    "To automate this process you can use a validation set. In this case we will use the last 48 hours of the training set to validate the score and select the best parameter value. This means that we will have to use a subset of the training set to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_generators = [sklearn.linear_model.LinearRegression, sklearn.linear_model.RidgeCV,\n",
    "                    sklearn.linear_model.LassoCV, sklearn.gaussian_process.GaussianProcess]\n",
    "best_score = 0\n",
    "\n",
    "##### Complete this part of the code #####\n",
    "for model_gen in :\n",
    "    for width in range(): \n",
    "##########################################        \n",
    "        X, y = convert_time_series_to_Xy(train_set, width)\n",
    "        # train the model on the first 48 hours\n",
    "        X_train, y_train = X[ : -48, :], y[ : -48]\n",
    "        # use the last 48 hours for validation\n",
    "        X_val, y_val = X[-48 : ], y[-48 : ]\n",
    "        \n",
    "        ##### Complete this part of the code #####\n",
    "        model = \n",
    "        ##########################################\n",
    "        \n",
    "        # there is a try except clause here because some model do not converge for some data\n",
    "        try:\n",
    "            ##### Complete this part of the code #####\n",
    "            model.fit()\n",
    "            this_score = model.score()\n",
    "            ##########################################\n",
    "            \n",
    "            if this_score > best_score:\n",
    "                best_score = this_score\n",
    "                best_model_gen = model_gen\n",
    "                best_width = width\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print best_model_gen().__class__, 'was selected as the best model with a width of', best_width, 'and a validation score of', best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct the LassoCV methods was selected.\n",
    "\n",
    "Now we are going to train this best model on all the data. In this way we use all the available data to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Complete this part of the code #####\n",
    "width = best_\n",
    "model = best_\n",
    "##########################################\n",
    "\n",
    "X, y = convert_time_series_to_Xy(train_set, width)\n",
    "\n",
    "##### Complete this part of the code #####\n",
    "model.fit() # train on the full data set\n",
    "##########################################\n",
    "\n",
    "nof_predictions = 200\n",
    "prediction = predict(model, train_set, width, nof_predictions)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((train_set, prediction[:,0])), 'g')\n",
    "plt.plot(train_set, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough the optimal result found here might not be the best visually, it is a far better result than the one you selected manually just because there was no cheating involved ;-).\n",
    "\n",
    "Some additional info:\n",
    "* This noise level of ```RidgeCV``` and ```LassoCV``` is estimated by automatically performing train and validation within the method itself. This will make them much more robust against over-fitting. The actual method used is [Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) which is a better approach of what we do here because it repeats the training and validation multiple times for different training and validation sets. The parameter that is set for these methods is often called the regularization parameter in literature and is well suited to avoid over-fitting.\n",
    "* Although sklearn supports estimating the noise level in Gaussian Processes it is not implemented within the method itself. Newer versions of sklearn seem to entail a lot of changes in this method so possibly it will be integrated in the (near) future. If you want to implement this noise level estimation yourself you can use [their cross-validation tool](http://scikit-learn.org/stable/modules/cross_validation.html) to set the [```alpha``` parameter](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) in version 0.18 of sklearn. (The version used here is 0.17.)\n",
    "\n",
    "\n",
    "\n",
    "## 5. Competition\n",
    "\n",
    "@BART mijn inspriratie is op..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
